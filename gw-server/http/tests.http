GET https://{{hostname}}:{{port}}/admin/llamacpp
Authorization: Bearer {{bearer_token}}

###

DELETE https://{{hostname}}:{{port}}/admin/llamacpp
Authorization: Bearer {{bearer_token}}

###

# devstral-small-2-24B
PUT https://{{hostname}}:{{port}}/admin/llamacpp
Authorization: Bearer {{bearer_token}}
Content-Type: application/json

{
    "env": {
        "GGML_CUDA_ENABLE_UNIFIED_MEMORY" : "1"
    },
    "alias" : "devstral-small-2-24B-instruct-2512",
    "model_path" : "/model_data/huggingface/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/Devstral-Small-2-24B-Instruct-2512-UD-Q8_K_XL.gguf",
    "mmproj_path" : "/model_data/huggingface/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/mmproj-F16.gguf",
    "prio" : 3,
    "min_p" : 0.01,
    "threads" : -1,
    "n_gpu_layers" : 99,
    "jinja" : true,
    "ctx_size" : 262144,
    "no_mmap" : true,
    "flash_attn" : "on"
}

###

# gpt-oss-120b-Q8_0
PUT https://{{hostname}}:{{port}}/admin/llamacpp
Authorization: Bearer {{bearer_token}}
Content-Type: application/json

{
    "env": {
        "GGML_CUDA_ENABLE_UNIFIED_MEMORY" : "1"
    },
    "alias" : "gpt-oss-120b-Q8_0",
    "model_path" : "/model_data/legacy/huggingface/Q8_0/gpt-oss-120b-Q8_0-00001-of-00002.gguf",
    "n_gpu_layers" : 99,
    "jinja" : true,
    "ctx_size" : 32768,
    "no_mmap" : true,
    "flash_attn" : "on",
    "batch_size" : 2048,
    "ubatch_size" : 2048,
    "cache_type_v" : "q8_0",
    "cache_type_k" : "q8_0",
    "parallel" : 1,
    "no_context_shift" : true,
    "no_cont_batching" : true
}
